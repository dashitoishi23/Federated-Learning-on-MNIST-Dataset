{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning using MNIST Dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3yhIpBOVRrY",
        "colab_type": "text"
      },
      "source": [
        "Federated Learning is a new revolutionary technique of training deep learning model according to which, models will learn on the client side instead of the server. The intuition behind this technique is that client devices will do local learning on the datasets and send the updated models to the central server in an aggregated form. \n",
        "\n",
        "Here I will create 2 virtual workers named \"Bob\" and \"Alice\" who will simulate real-world client side devices as far as training data locally on client side devices is concerned. Using the excellent PySyft module, we can simulate that experience.\n",
        "\n",
        "I will use FederatedDataLoader instead of the standard DataLoader to sort of decentralize the data to both Bob and Alice\n",
        "\n",
        "Federated learning is used most intensively by Google in their Gboard app which they use for text prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0IYAJaFI5BY",
        "colab_type": "code",
        "outputId": "4e30fe72-cbcf-438b-c29d-1c48eca4322a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import syft as sy  #import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # Hooking PyTorch to PySyft\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # Creating workers\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader( # using a FederatedDataLoader instead of a normal DataLoader\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), #sending dataset too both workers\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "      \n",
        "criterion = nn.CrossEntropyLoss();\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): \n",
        "        model.send(data.location) #send the model to both workers as per location\n",
        "        data = data.view(data.shape[0],-1)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() \n",
        "        if batch_idx % 1000 == 0:\n",
        "            loss = loss.get() \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * 64, len(train_loader) * 64, \\\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.view(data.shape[0],-1)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            ps = torch.exp(output) #finding the probablity distribution for an image\n",
        "            top_p,top_class = ps.topk(1,dim=1) #find the class that the model predicted\n",
        "            equals = top_class == target.view(*top_class.shape)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {} ({:.0f}%)\\n'.format(\n",
        "        test_loss, torch.mean(equals.type(torch.FloatTensor)),\n",
        "        100.*torch.mean(equals.type(torch.FloatTensor))))\n",
        "    \n",
        "    \n",
        "model = nn.Sequential(nn.Linear(784,512),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(512,256),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(256,10),\n",
        "                     nn.LogSoftmax(dim=1))\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001) \n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train(model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 19:56:07.748313 140709234329472 hook.py:97] Torch was already hooked... skipping hooking process\n",
            "/usr/local/lib/python3.6/dist-packages/syft/workers/base.py:385: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  response = command(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.318359\n",
            "\n",
            "Test set: Average loss: 0.0317, Accuracy: 0.6875 (69%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 1.990206\n",
            "\n",
            "Test set: Average loss: 0.0228, Accuracy: 0.75 (75%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 1.441642\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 0.8125 (81%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.958632\n",
            "\n",
            "Test set: Average loss: 0.0113, Accuracy: 0.9375 (94%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.756255\n",
            "\n",
            "Test set: Average loss: 0.0093, Accuracy: 0.6875 (69%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.606392\n",
            "\n",
            "Test set: Average loss: 0.0081, Accuracy: 0.6875 (69%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.429412\n",
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 0.875 (88%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.412929\n",
            "\n",
            "Test set: Average loss: 0.0068, Accuracy: 0.9375 (94%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.487510\n",
            "\n",
            "Test set: Average loss: 0.0064, Accuracy: 0.875 (88%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.331846\n",
            "\n",
            "Test set: Average loss: 0.0061, Accuracy: 0.8125 (81%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}